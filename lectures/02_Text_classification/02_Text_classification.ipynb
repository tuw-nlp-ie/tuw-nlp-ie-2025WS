{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "\n",
    "### Natural Language Processing and Information Extraction,  2024WS\n",
    "Lecture 2, 10/17/2025\n",
    "\n",
    "GÃ¡bor Recski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this lecture\n",
    "\n",
    "- Text classification\n",
    "    - Examples of text classification\n",
    "    - Naive Bayes classification (previous SLP 4.1, 4.2)\n",
    "    - Evaluation (SLP 4.9)\n",
    "    - Example: sentiment analysis (SLP 4.3)\n",
    "    - Error analysis / model debugging\n",
    "- N-gram language modelling (SLP 3.1)\n",
    "\n",
    "[previous SLP Ch. 4](https://web.stanford.edu/~jurafsky/slp3/old_aug24/4.pdf)\n",
    "[SLP Ch. 4](https://web.stanford.edu/~jurafsky/slp3/4.pdf)\n",
    "[SLP Ch. 3](https://web.stanford.edu/~jurafsky/slp3/3.pdf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import Counter, defaultdict\n",
    "from math import exp, log\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification tasks are some of the simplest and most common NLP applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks may involve classifying **documents**, **sentences** or **words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-level classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sa](media/sa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spam detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spam](media/spam.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence-/utterance-level classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intent detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![intent](media/intent.png)\n",
    "\n",
    "([SentiOne](https://sentione.com/blog/new-state-of-the-art-intent-detection-model-from-sentione))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-level classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slot-filling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![slot](media/slot.png)\n",
    "\n",
    "([Hung-yi Lee's Slides](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/RNN%20(v2).pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic representation of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bow](media/bow.png)\n",
    "\n",
    "([SLP Ch.4](https://web.stanford.edu/~jurafsky/slp3/4.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the bag-of-words representation for learning to classify documents.\n",
    "Each document in a training dataset will be represented by its words: $d_i = (x_1, x_2, \\ldots, x_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a document $d$ and class $c$, maximize the likelihood that $d$ belongs to $c$ over all classes $c\\in C$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\underset{c\\in C}{\\operatorname{argmax}}P(c|d)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply [Bayes' rule](https://en.wikipedia.org/wiki/Bayes%27_theorem):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(c|d) = \\frac{P(d|c)P(c)}{P(d)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When classifying a document, the denominator is the same for all classes and can be dropped:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\underset{c\\in C}{\\operatorname{argmax}}P(d|c)P(c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a document is represented by features $x_1, x_2, \\ldots x_n$ then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(d|c) = P(x_1, x_2, \\ldots, x_n|c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we make an **assumption** that the features are independent, then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(x_1, x_2, \\ldots, x_n|c) = P(x_1|c)P(x_2|c)\\ldots P(x_n|c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the bag-of-words model, the features are the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$c_{\\text{NB}} = \\underset{c_j\\in C}{\\operatorname{argmax}}P(c_j)\\prod_{w_i \\in D}{P(w_i|c)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid underflow errors, we usually implement this in log-space:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$c_{\\text{NB}} = \\underset{c_j\\in C}{\\operatorname{argmax}}\\log P(c_j)+\\sum_{w_i \\in D}{\\log P(w_i|c)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And instead of multiplying very small numbers we are now adding up weights. The class with the highest score is still the most likely class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we estimate (learn) the paramteres $P(w_i|c_j)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way is by counting occurrences in the training data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(w_i|c_j) = \\frac{\\text{count}(w_i, c_j)}{\\sum_{w\\in V}{\\text{count}(w, c_j)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $V$ is the **vocabulary** of all words in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a word is not in the vocabulary (out-of-vocabulary, OOV), we don't want to introduce zero probabilities. The simplest solution is Laplace (add-1) **smoothing**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(w_i|c_j) = \\frac{\\text{count}(w_i, c_j)+1}{\\sum_{w\\in V}{(\\text{count}(w, c_j)+1)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another simple solution is to leave OOVs out of the equation completely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An implementation of a Naive Bayes classifier with this simplest approach will follow later in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes may be naive, but it is powerful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- very fast, with low memory requirements\n",
    "- robust to irrelevant features\n",
    "- works well in domains with many equally important features\n",
    "- a strong, dependable baseline for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bag-of-words may be naive, but it is powerful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- efficient: vocabularies are large but number of relevant dimensions is limited\n",
    "- interpretable: lets us understand what a model learned\n",
    "- robust, low-resource, does not depend on genre, domain, language-specific resources, etc.\n",
    "\n",
    "**In natural language, most of the information comes from words!**\n",
    "\n",
    "(The word entropy of natural language is 12-16 bits ([Kornai 2008](https://eprints.sztaki.hu/7913/1/Kornai_1762289_ny.pdf) Section 7.1), punctuation doesn't contribute much (7% for English, see [Brown 1992](https://aclanthology.org/J92-1002.pdf)), and syntax cannot contribute more than 2 bits per word ($C_n\\sim 4^n$, where $C_n$ are the [Catalan numbers](https://en.wikipedia.org/wiki/Catalan_number)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eval](media/eval.png)\n",
    "\n",
    "([SLP Ch.4](https://web.stanford.edu/~jurafsky/slp3/4.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F-measure** is the (weighted) harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F_\\beta = \\frac{(\\beta^2+1)PR}{\\beta^2P+R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced F-measure ($\\beta=1$):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F_1 = \\frac{2PR}{P+R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 is used everywhere in **academia** (challenges, leaderboards), but precision and recall are usually **not equally important in real-world applications**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small differences in F-score may not mean small differences in output. You should **always look at precision and recall separately**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eval2](media/eval2.png)\n",
    "\n",
    "([SLP Ch.4](https://web.stanford.edu/~jurafsky/slp3/4.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macro- vs. micro-averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eval3](media/eval3.png)\n",
    "\n",
    "([SLP Ch.4](https://web.stanford.edu/~jurafsky/slp3/4.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a BOW classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNBClassifier():\n",
    "    def __init__(self):\n",
    "        self.word_count = Counter()\n",
    "        self.count_by_class = defaultdict(Counter)\n",
    "        self.labels = set()\n",
    "    \n",
    "    def count_words(self, docs):\n",
    "        for words, label in docs:\n",
    "            self.labels.add(label)\n",
    "            for word in set(words):\n",
    "                self.word_count[word] += 1\n",
    "                self.count_by_class[label][word] += 1\n",
    "    \n",
    "    def calculate_weights(self):\n",
    "        self.weights = {\n",
    "            word: {\n",
    "                label: log((self.count_by_class[label][word] + 1) / (count + len(self.word_count)))\n",
    "                for label in self.labels}\n",
    "            for word, count in self.word_count.items()}\n",
    "\n",
    "    def get_doc_weights(self, doc):\n",
    "        return {\n",
    "            label: sum(\n",
    "                self.weights[word][label] if word in self.weights else log(1)\n",
    "                for word in doc)\n",
    "            for label in self.labels}\n",
    "    \n",
    "    def predict_label(self, doc):\n",
    "        doc_weights = self.get_doc_weights(doc)\n",
    "        return sorted(doc_weights.items(), key=lambda x: -x[1])[0][0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** this implementation is not very efficient, normally you would use an ML library with efficient data structures and algorithms. E.g. here you could have used [scikit-learn's NB implementation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = SimpleNBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = [\n",
    "    ((\"He\", \"is\", \"nice\"), \"POS\"),\n",
    "    ((\"He\", \"is\", \"stupid\"), \"NEG\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.count_words(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.calculate_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'He': {'NEG': -1.0986122886681098, 'POS': -1.0986122886681098},\n",
       " 'is': {'NEG': -1.0986122886681098, 'POS': -1.0986122886681098},\n",
       " 'nice': {'NEG': -1.6094379124341003, 'POS': -0.916290731874155},\n",
       " 'stupid': {'NEG': -0.916290731874155, 'POS': -1.6094379124341003}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = ('This', 'movie', 'is', 'stupid')\n",
    "doc2 = ('This', 'movie', 'is', 'nice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NEG': -2.0149030205422647, 'POS': -2.70805020110221}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.get_doc_weights(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEG'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.predict_label(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NEG': -2.70805020110221, 'POS': -2.0149030205422647}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.get_doc_weights(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POS'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.predict_label(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a real dataset: [50k movie reviews from IMDB](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [01:11, 701.03it/s]\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "with open('data/imdb_dataset.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for text, label in tqdm(reader):\n",
    "        words = nltk.word_tokenize(text)        \n",
    "        docs.append((words, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = SimpleNBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('positive', 25000), ('negative', 25000)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter((d[1] for d in docs)).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.count_words(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.calculate_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"This movie is stupid\",\n",
    "    \"I loved this movie\",\n",
    "    \"This is the most famous movie by Aaron Sorkin\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This', 'movie', 'is', 'stupid'],\n",
       " ['I', 'loved', 'this', 'movie'],\n",
       " ['This', 'is', 'the', 'most', 'famous', 'movie', 'by', 'Aaron', 'Sorkin']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_docs = [nltk.word_tokenize(sen) for sen in sentences]\n",
    "sample_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'positive', 'positive']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [nb.predict_label(doc) for doc in sample_docs]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'positive': -14.242144543286088, 'negative': -12.541662089933203},\n",
       " {'positive': -12.507978373722635, 'negative': -13.287294327272875},\n",
       " {'positive': -39.82907148625904, 'negative': -42.89235870322001}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [nb.get_doc_weights(doc) for doc in sample_docs]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_word_weights(doc, model):\n",
    "    print(('{:>12}'*4).format('word', 'pos', 'neg', 'diff'))\n",
    "    print('='*48)\n",
    "    threshold = 1\n",
    "    for word in doc:\n",
    "        if word in model.weights:\n",
    "            pos_weight, neg_weight = model.weights[word]['positive'], model.weights[word]['negative']\n",
    "            diff = pos_weight - neg_weight\n",
    "        else:\n",
    "            pos_weight, neg_weight, diff = -1, -1, 0\n",
    "        p_word = word if abs(diff) < threshold else f\"*{word}\"\n",
    "        print(f'{p_word:>12}{pos_weight:>12.2f}{neg_weight:>12.2f}{diff:>12.2f}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word         pos         neg        diff\n",
      "================================================\n",
      "        This       -3.02       -3.02        0.00\n",
      "       movie       -2.80       -2.63       -0.17\n",
      "          is       -2.36       -2.38        0.02\n",
      "     *stupid       -6.07       -4.52       -1.55\n"
     ]
    }
   ],
   "source": [
    "show_word_weights(sample_docs[0], nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word         pos         neg        diff\n",
      "================================================\n",
      "           I       -2.54       -2.46       -0.08\n",
      "      *loved       -4.72       -5.81        1.10\n",
      "        this       -2.46       -2.39       -0.07\n",
      "       movie       -2.80       -2.63       -0.17\n"
     ]
    }
   ],
   "source": [
    "show_word_weights(sample_docs[1], nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word         pos         neg        diff\n",
      "================================================\n",
      "        This       -3.02       -3.02        0.00\n",
      "          is       -2.36       -2.38        0.02\n",
      "         the       -2.29       -2.29       -0.00\n",
      "        most       -3.46       -3.62        0.16\n",
      "      famous       -5.49       -6.05        0.56\n",
      "       movie       -2.80       -2.63       -0.17\n",
      "          by       -2.92       -2.97        0.05\n",
      "       Aaron       -7.88       -8.44        0.57\n",
      "     *Sorkin       -9.61      -11.49        1.87\n"
     ]
    }
   ],
   "source": [
    "show_word_weights(sample_docs[2], nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train and evaluate a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs, dev_docs, test_docs = docs[:40000], docs[40001:45000], docs[45001:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = SimpleNBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.count_words(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.calculate_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    tp, fp, tn, fn = 0, 0, 0, 0\n",
    "    for doc, label in data:\n",
    "        pred_label = model.predict_label(doc)\n",
    "        if pred_label == 'positive':\n",
    "            if label == 'positive':\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        else:\n",
    "            if label == 'positive':\n",
    "                fn += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "    print(f\"TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}\")\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = (2*precision*recall) / (precision + recall)\n",
    "    print(f'Precision: {precision:5.2%}, Recall: {recall:5.2%}, F1: {f1:5.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 1943, FP: 219, FN: 594, TN: 2243\n",
      "Precision: 89.87%, Recall: 76.59%, F1: 82.70%\n"
     ]
    }
   ],
   "source": [
    "evaluate(nb, dev_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis / model debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to understand the model's behavior and why it makes errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dev_docs[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 4, FP: 1, FN: 3, TN: 12\n",
      "Precision: 80.00%, Recall: 57.14%, F1: 66.67%\n"
     ]
    }
   ],
   "source": [
    "evaluate(nb, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors(model, data):\n",
    "    FPs, FNs = [], []\n",
    "    for doc, gold_label in data:\n",
    "        pred_label = nb.predict_label(doc)\n",
    "        if pred_label == gold_label:\n",
    "            continue\n",
    "        if pred_label == 'positive':\n",
    "            FPs.append(doc)\n",
    "        else:\n",
    "            FNs.append(doc)\n",
    "    return FPs, FNs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPs, FNs = get_errors(nb, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word         pos         neg        diff\n",
      "================================================\n",
      "   Hitchcock       -6.87       -7.32        0.45\n",
      "          is       -2.45       -2.46        0.02\n",
      "           a       -2.40       -2.39       -0.00\n",
      "       great       -3.36       -4.09        0.73\n",
      "    director       -4.47       -4.25       -0.22\n",
      "           .       -2.38       -2.37       -0.00\n",
      "  Ironically       -8.05       -8.53        0.48\n",
      "           I       -2.63       -2.55       -0.08\n",
      "      mostly       -5.59       -5.63        0.04\n",
      "        find       -4.13       -4.28        0.15\n",
      "         his       -3.07       -3.20        0.13\n",
      "       films       -3.83       -4.02        0.19\n",
      "           a       -2.40       -2.39       -0.00\n",
      "       total       -6.45       -5.63       -0.82\n",
      "      *waste       -7.28       -4.56       -2.71\n",
      "          of       -2.41       -2.41       -0.00\n",
      "        time       -3.36       -3.35       -0.01\n",
      "          to       -2.43       -2.41       -0.02\n",
      "       watch       -3.84       -3.78       -0.06\n",
      "           .       -2.38       -2.37       -0.00\n",
      "           I       -2.63       -2.55       -0.08\n",
      "      admire       -7.36       -7.74        0.38\n",
      "    Hitchcok      -11.37      -10.96       -0.41\n",
      "          on       -2.79       -2.76       -0.03\n",
      "           a       -2.40       -2.39       -0.00\n",
      "      purely       -7.32       -7.23       -0.08\n",
      "      visual       -5.95       -6.45        0.49\n",
      "         and       -2.39       -2.41        0.02\n",
      "   technical       -6.83       -6.79       -0.04\n",
      "      level.       -9.02       -8.84       -0.17\n",
      "           <       -2.84       -2.80       -0.04\n",
      "          br       -2.84       -2.80       -0.04\n",
      "           /       -2.84       -2.80       -0.04\n",
      "           >       -2.84       -2.80       -0.04\n",
      "           <       -2.84       -2.80       -0.04\n",
      "          br       -2.84       -2.80       -0.04\n",
      "           /       -2.84       -2.80       -0.04\n",
      "           >       -2.84       -2.80       -0.04\n",
      "       First       -5.85       -5.30       -0.55\n",
      "         the       -2.38       -2.37       -0.00\n",
      "  *positives       -9.76       -8.53       -1.22\n",
      "           .       -2.38       -2.37       -0.00\n",
      "   Hitchcock       -6.87       -7.32        0.45\n",
      "    invented       -8.15       -7.90       -0.25\n",
      "         the       -2.38       -2.37       -0.00\n",
      "      notion       -7.65       -7.87        0.22\n",
      "          of       -2.41       -2.41       -0.00\n",
      "         the       -2.38       -2.37       -0.00\n",
      "     probing       -9.50       -9.98        0.49\n",
      "      camera       -5.33       -4.93       -0.40\n",
      "           .       -2.38       -2.37       -0.00\n",
      "         The       -2.69       -2.64       -0.04\n",
      "     curious       -6.92       -6.90       -0.02\n",
      "         eye       -5.84       -6.20        0.35\n",
      "        that       -2.57       -2.53       -0.04\n",
      "          is       -2.45       -2.46        0.02\n",
      "        able       -5.12       -5.37        0.25\n",
      "          to       -2.43       -2.41       -0.02\n",
      "    withhold      -10.67      -11.37        0.69\n",
      "          or       -3.27       -3.06       -0.21\n",
      "      search       -6.50       -6.75        0.25\n",
      "         for       -2.67       -2.66       -0.01\n",
      " information       -6.36       -6.75        0.39\n",
      "           .       -2.38       -2.37       -0.00\n",
      "          It       -3.04       -3.09        0.04\n",
      "          is       -2.45       -2.46        0.02\n",
      "         n't       -2.94       -2.70       -0.23\n",
      "     exactly       -5.57       -5.44       -0.13\n",
      "           a       -2.40       -2.39       -0.00\n",
      "         new       -4.46       -4.69        0.23\n",
      "      visual       -5.95       -6.45        0.49\n",
      "     grammar       -9.50       -9.12       -0.38\n",
      "         but       -2.74       -2.69       -0.05\n",
      "          it       -2.52       -2.50       -0.02\n",
      "         was       -2.78       -2.67       -0.11\n",
      "revolutionary       -7.77       -8.13        0.36\n",
      "       then.       -9.35       -9.06       -0.29\n",
      "           <       -2.84       -2.80       -0.04\n",
      "          br       -2.84       -2.80       -0.04\n",
      "           /       -2.84       -2.80       -0.04\n",
      "           >       -2.84       -2.80       -0.04\n",
      "           <       -2.84       -2.80       -0.04\n",
      "          br       -2.84       -2.80       -0.04\n",
      "           /       -2.84       -2.80       -0.04\n",
      "           >       -2.84       -2.80       -0.04\n",
      "    Secondly       -8.30       -7.39       -0.91\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "   Hitchcock       -6.87       -7.32        0.45\n",
      "      pretty       -4.56       -4.23       -0.34\n",
      "        much       -3.57       -3.47       -0.11\n",
      "  *perfected       -8.63       -9.86        1.24\n",
      "         the       -2.38       -2.37       -0.00\n",
      "    thriller       -5.80       -5.97        0.17\n",
      "         and       -2.39       -2.41        0.02\n",
      "       chase       -6.68       -6.53       -0.16\n",
      "       movie       -2.89       -2.72       -0.17\n",
      "           .       -2.38       -2.37       -0.00\n",
      "          He       -4.08       -4.28        0.20\n",
      "         has       -3.11       -3.21        0.11\n",
      "          an       -2.99       -3.02        0.03\n",
      "  economical       -9.35      -10.27        0.92\n",
      "       style       -5.09       -5.39        0.29\n",
      "         and       -2.39       -2.41        0.02\n",
      "          is       -2.45       -2.46        0.02\n",
      "      always       -4.23       -4.77        0.54\n",
      "    thinking       -5.64       -5.16       -0.48\n",
      "          of       -2.41       -2.41       -0.00\n",
      "         the       -2.38       -2.37       -0.00\n",
      "    audience       -4.89       -4.83       -0.06\n",
      "           .       -2.38       -2.37       -0.00\n",
      "          He       -4.08       -4.28        0.20\n",
      "       gives       -4.83       -5.42        0.59\n",
      "        them       -3.76       -3.70       -0.06\n",
      "     regular       -6.76       -6.96        0.20\n",
      "     thrills       -7.65       -7.46       -0.20\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "     regular       -6.76       -6.96        0.20\n",
      "       jolts      -10.45       -9.58       -0.88\n",
      "          of       -2.41       -2.41       -0.00\n",
      "      humour       -6.18       -6.50        0.31\n",
      "         and       -2.39       -2.41        0.02\n",
      "     regular       -6.76       -6.96        0.20\n",
      "      shocks       -8.32       -8.45        0.13\n",
      "           .       -2.38       -2.37       -0.00\n",
      "          In       -3.88       -3.94        0.06\n",
      "       short       -4.96       -5.05        0.09\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "          he       -3.20       -3.24        0.04\n",
      " anticipates       -9.86      -10.67        0.81\n",
      "         the       -2.38       -2.37       -0.00\n",
      "    audience       -4.89       -4.83       -0.06\n",
      "          's       -2.65       -2.66        0.02\n",
      "        base       -7.62       -7.18       -0.44\n",
      "       needs       -5.73       -5.66       -0.07\n",
      "         and       -2.39       -2.41        0.02\n",
      "       plays       -4.63       -5.01        0.38\n",
      "        them       -3.76       -3.70       -0.06\n",
      "        like       -3.17       -2.98       -0.19\n",
      "           a       -2.40       -2.39       -0.00\n",
      "     fiddle.       -1.00       -1.00        0.00\n",
      "           <       -2.84       -2.80       -0.04\n",
      "          br       -2.84       -2.80       -0.04\n",
      "           /       -2.84       -2.80       -0.04\n",
      "           >       -2.84       -2.80       -0.04\n",
      "           <       -2.84       -2.80       -0.04\n",
      "          br       -2.84       -2.80       -0.04\n",
      "           /       -2.84       -2.80       -0.04\n",
      "           >       -2.84       -2.80       -0.04\n",
      "Unfortunately       -6.22       -5.34       -0.88\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "         the       -2.38       -2.37       -0.00\n",
      "        base       -7.62       -7.18       -0.44\n",
      "       needs       -5.73       -5.66       -0.07\n",
      "          of       -2.41       -2.41       -0.00\n",
      "           a       -2.40       -2.39       -0.00\n",
      "       human       -5.00       -5.50        0.50\n",
      "       being       -3.85       -3.82       -0.03\n",
      "         are       -2.86       -2.86        0.00\n",
      "      mostly       -5.59       -5.63        0.04\n",
      "     *stupid       -6.16       -4.63       -1.53\n",
      "           .       -2.38       -2.37       -0.00\n",
      "        Food       -9.29       -9.23       -0.06\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "         sex       -5.55       -5.12       -0.42\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "         the       -2.38       -2.37       -0.00\n",
      "      thrill       -7.61       -8.00        0.39\n",
      "          of       -2.41       -2.41       -0.00\n",
      "      danger       -7.12       -7.23        0.11\n",
      "         and       -2.39       -2.41        0.02\n",
      "           a       -2.40       -2.39       -0.00\n",
      "      little       -3.89       -3.95        0.06\n",
      "      comedy       -4.53       -4.73        0.19\n",
      "           .       -2.38       -2.37       -0.00\n",
      "    Hithcock      -11.37      -12.06        0.69\n",
      "      caters       -9.76      -10.45        0.69\n",
      "         for       -2.67       -2.66       -0.01\n",
      "         all       -3.00       -2.97       -0.03\n",
      "       these       -4.17       -4.08       -0.08\n",
      "       needs       -5.73       -5.66       -0.07\n",
      "          on       -2.79       -2.76       -0.03\n",
      "      screen       -4.73       -4.80        0.08\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "        with       -2.67       -2.69        0.02\n",
      "         the       -2.38       -2.37       -0.00\n",
      "   exception       -6.60       -6.33       -0.26\n",
      "          of       -2.41       -2.41       -0.00\n",
      "        food       -6.72       -6.77        0.05\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "       which       -3.45       -3.46        0.01\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "     judging       -8.11       -7.72       -0.39\n",
      "        from       -3.04       -3.06        0.02\n",
      "         his       -3.07       -3.20        0.13\n",
      "        size       -7.80       -7.32       -0.48\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "          he       -3.20       -3.24        0.04\n",
      "     catered      -10.45      -10.27       -0.18\n",
      "          to       -2.43       -2.41       -0.02\n",
      "         off       -4.16       -3.81       -0.35\n",
      "     screen.       -7.69       -7.86        0.16\n",
      "           <       -2.84       -2.80       -0.04\n",
      "          br       -2.84       -2.80       -0.04\n",
      "           /       -2.84       -2.80       -0.04\n",
      "           >       -2.84       -2.80       -0.04\n",
      "           <       -2.84       -2.80       -0.04\n",
      "          br       -2.84       -2.80       -0.04\n",
      "           /       -2.84       -2.80       -0.04\n",
      "           >       -2.84       -2.80       -0.04\n",
      "          It       -3.04       -3.09        0.04\n",
      "          's       -2.65       -2.66        0.02\n",
      "        this       -2.54       -2.47       -0.07\n",
      "   pandering      -10.27       -9.50       -0.77\n",
      "          to       -2.43       -2.41       -0.02\n",
      "         the       -2.38       -2.37       -0.00\n",
      "    audience       -4.89       -4.83       -0.06\n",
      "        that       -2.57       -2.53       -0.04\n",
      "   sabotages      -10.45      -10.45        0.00\n",
      "        most       -3.56       -3.72        0.16\n",
      "          of       -2.41       -2.41       -0.00\n",
      "         his       -3.07       -3.20        0.13\n",
      "       films       -3.83       -4.02        0.19\n",
      "           .       -2.38       -2.37       -0.00\n",
      "           A       -3.78       -3.92        0.14\n",
      "      second       -4.98       -5.12        0.14\n",
      "   *downside       -8.63       -9.76        1.13\n",
      "          is       -2.45       -2.46        0.02\n",
      "        that       -2.57       -2.53       -0.04\n",
      "        most       -3.56       -3.72        0.16\n",
      "          of       -2.41       -2.41       -0.00\n",
      "   Hitchcock       -6.87       -7.32        0.45\n",
      "          's       -2.65       -2.66        0.02\n",
      "      camera       -5.33       -4.93       -0.40\n",
      "        work       -4.20       -4.26        0.06\n",
      "         and       -2.39       -2.41        0.02\n",
      "      visual       -5.95       -6.45        0.49\n",
      "     grammar       -9.50       -9.12       -0.38\n",
      "         are       -2.86       -2.86        0.00\n",
      "         now       -4.30       -4.48        0.18\n",
      "      common       -6.11       -6.20        0.09\n",
      "       place       -4.72       -4.75        0.03\n",
      "           .       -2.38       -2.37       -0.00\n",
      "        What       -4.58       -4.23       -0.35\n",
      "       keeps       -5.68       -6.23        0.55\n",
      "         his       -3.07       -3.20        0.13\n",
      "       films       -3.83       -4.02        0.19\n",
      "   watchable       -7.18       -6.35       -0.83\n",
      "         are       -2.86       -2.86        0.00\n",
      "         the       -2.38       -2.37       -0.00\n",
      "      simple       -5.25       -5.91        0.66\n",
      "     economy       -8.37       -9.12        0.74\n",
      "          of       -2.41       -2.41       -0.00\n",
      "         his       -3.07       -3.20        0.13\n",
      "       tales       -7.44       -8.30        0.86\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "         the       -2.38       -2.37       -0.00\n",
      "intelligence       -6.89       -6.54       -0.35\n",
      "          of       -2.41       -2.41       -0.00\n",
      "         his       -3.07       -3.20        0.13\n",
      "      camera       -5.33       -4.93       -0.40\n",
      "        work       -4.20       -4.26        0.06\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "         and       -2.39       -2.41        0.02\n",
      "         his       -3.07       -3.20        0.13\n",
      "       skill       -7.09       -7.34        0.26\n",
      "          at       -3.02       -2.94       -0.08\n",
      "   *crafting       -9.29      -10.45        1.16\n",
      "       tense       -7.02       -7.86        0.84\n",
      "      action       -4.66       -4.66        0.00\n",
      "         set       -4.75       -4.82        0.07\n",
      "     pieces.       -9.98       -9.98        0.00\n",
      "           <       -2.84       -2.80       -0.04\n",
      "          br       -2.84       -2.80       -0.04\n",
      "           /       -2.84       -2.80       -0.04\n",
      "           >       -2.84       -2.80       -0.04\n",
      "           <       -2.84       -2.80       -0.04\n",
      "          br       -2.84       -2.80       -0.04\n",
      "           /       -2.84       -2.80       -0.04\n",
      "           >       -2.84       -2.80       -0.04\n",
      "          So       -4.72       -4.47       -0.26\n",
      "          on       -2.79       -2.76       -0.03\n",
      "          to       -2.43       -2.41       -0.02\n",
      "          ``       -3.08       -2.98       -0.10\n",
      "   *Saboteur       -9.50      -10.67        1.18\n",
      "          ''       -3.08       -2.98       -0.10\n",
      "           .       -2.38       -2.37       -0.00\n",
      "        This       -3.12       -3.11       -0.01\n",
      "          is       -2.45       -2.46        0.02\n",
      "           a       -2.40       -2.39       -0.00\n",
      "*light-hearted       -7.70       -8.93        1.22\n",
      "        romp       -7.84       -8.28        0.44\n",
      "          in       -2.48       -2.49        0.01\n",
      "         the       -2.38       -2.37       -0.00\n",
      "        vein       -8.04       -8.15        0.11\n",
      "          of       -2.41       -2.41       -0.00\n",
      "          ``       -3.08       -2.98       -0.10\n",
      "         The       -2.69       -2.64       -0.04\n",
      "          39       -9.12       -9.23        0.11\n",
      "       Steps       -9.23       -9.76        0.53\n",
      "          ''       -3.08       -2.98       -0.10\n",
      "           .       -2.38       -2.37       -0.00\n",
      "          It       -3.04       -3.09        0.04\n",
      "       jumps       -7.62       -7.23       -0.39\n",
      "        from       -3.04       -3.06        0.02\n",
      "    sequence       -5.67       -5.84        0.17\n",
      "          to       -2.43       -2.41       -0.02\n",
      "    sequence       -5.67       -5.84        0.17\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "       until       -4.90       -5.06        0.16\n",
      "          it       -2.52       -2.50       -0.02\n",
      "   concludes       -8.63       -8.97        0.34\n",
      "          at       -3.02       -2.94       -0.08\n",
      "         the       -2.38       -2.37       -0.00\n",
      "     typical       -5.67       -5.94        0.26\n",
      "   Hitchcock       -6.87       -7.32        0.45\n",
      "       final       -5.23       -5.45        0.22\n",
      "         act       -5.52       -5.22       -0.30\n",
      "         set       -4.75       -4.82        0.07\n",
      "      piece.       -9.50       -9.98        0.49\n",
      "           <       -2.84       -2.80       -0.04\n",
      "          br       -2.84       -2.80       -0.04\n",
      "           /       -2.84       -2.80       -0.04\n",
      "           >       -2.84       -2.80       -0.04\n",
      "           <       -2.84       -2.80       -0.04\n",
      "          br       -2.84       -2.80       -0.04\n",
      "           /       -2.84       -2.80       -0.04\n",
      "           >       -2.84       -2.80       -0.04\n",
      "          On       -5.51       -5.44       -0.07\n",
      "          an       -2.99       -3.02        0.03\n",
      "   emotional       -5.65       -6.39        0.74\n",
      "       level       -5.68       -5.58       -0.10\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "         the       -2.38       -2.37       -0.00\n",
      "relationship       -5.38       -6.03        0.65\n",
      "     between       -4.28       -4.58        0.30\n",
      "         the       -2.38       -2.37       -0.00\n",
      "       leads       -5.66       -5.96        0.30\n",
      "          is       -2.45       -2.46        0.02\n",
      "         not       -2.89       -2.80       -0.09\n",
      "          up       -3.40       -3.31       -0.09\n",
      "          to       -2.43       -2.41       -0.02\n",
      "         par       -7.44       -7.11       -0.33\n",
      "        with       -2.67       -2.69        0.02\n",
      "      Robert       -5.49       -5.78        0.29\n",
      "       Donat      -10.45      -11.37        0.92\n",
      "         and       -2.39       -2.41        0.02\n",
      "   Madeleine       -8.73       -9.66        0.93\n",
      "      Caroll      -10.45      -11.37        0.92\n",
      "          in       -2.48       -2.49        0.01\n",
      "          ``       -3.08       -2.98       -0.10\n",
      "         The       -2.69       -2.64       -0.04\n",
      "          39       -9.12       -9.23        0.11\n",
      "       steps       -7.25       -7.68        0.43\n",
      "          ''       -3.08       -2.98       -0.10\n",
      "           .       -2.38       -2.37       -0.00\n",
      "       Hence       -8.84       -8.32       -0.52\n",
      "         the       -2.38       -2.37       -0.00\n",
      "       whole       -4.62       -4.31       -0.31\n",
      "       story       -3.38       -3.59        0.20\n",
      "       lacks       -7.07       -6.10       -0.96\n",
      "           a       -2.40       -2.39       -0.00\n",
      "     certain       -5.59       -5.86        0.26\n",
      "      energy       -6.70       -6.71        0.01\n",
      "           .       -2.38       -2.37       -0.00\n",
      "         The       -2.69       -2.64       -0.04\n",
      "        plot       -4.15       -3.63       -0.52\n",
      "      simply       -5.07       -4.80       -0.27\n",
      "     rumbles      -11.37      -10.45       -0.92\n",
      "          on       -2.79       -2.76       -0.03\n",
      "        like       -3.17       -2.98       -0.19\n",
      "           a       -2.40       -2.39       -0.00\n",
      "     machine       -7.07       -6.74       -0.32\n",
      "           ,       -2.40       -2.39       -0.01\n",
      " desperately       -7.35       -7.14       -0.21\n",
      "   depending       -8.00       -8.48        0.48\n",
      "          on       -2.79       -2.76       -0.03\n",
      "         the       -2.38       -2.37       -0.00\n",
      "    addition       -6.39       -6.70        0.31\n",
      "          of       -2.41       -2.41       -0.00\n",
      "         new       -4.46       -4.69        0.23\n",
      "      scenes       -4.14       -4.01       -0.13\n",
      "           .       -2.38       -2.37       -0.00\n",
      "         And       -3.95       -3.79       -0.16\n",
      "         new       -4.46       -4.69        0.23\n",
      "      scenes       -4.14       -4.01       -0.13\n",
      "        only       -3.51       -3.26       -0.25\n",
      "       bring       -5.62       -5.78        0.16\n",
      "          us       -4.42       -4.69        0.27\n",
      "      nearer      -10.45      -10.11       -0.34\n",
      "         the       -2.38       -2.37       -0.00\n",
      "         end       -4.00       -3.96       -0.05\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "       since       -4.57       -4.63        0.06\n",
      "          it       -2.52       -2.50       -0.02\n",
      "          's       -2.65       -2.66        0.02\n",
      "         not       -2.89       -2.80       -0.09\n",
      "       clear       -5.78       -5.65       -0.12\n",
      "          if       -3.48       -3.25       -0.23\n",
      "         the       -2.38       -2.37       -0.00\n",
      "        hook       -7.97       -7.76       -0.21\n",
      "          is       -2.45       -2.46        0.02\n",
      "         the       -2.38       -2.37       -0.00\n",
      "        hero       -5.76       -5.71       -0.05\n",
      "          's       -2.65       -2.66        0.02\n",
      "      escape       -6.03       -6.34        0.30\n",
      "        from       -3.04       -3.06        0.02\n",
      "         the       -2.38       -2.37       -0.00\n",
      "      police       -5.79       -5.79        0.00\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "        from       -3.04       -3.06        0.02\n",
      "         the       -2.38       -2.37       -0.00\n",
      "    villains       -6.75       -7.03        0.28\n",
      "          or       -3.27       -3.06       -0.21\n",
      "         his       -3.07       -3.20        0.13\n",
      "      action       -4.66       -4.66        0.00\n",
      "          to       -2.43       -2.41       -0.02\n",
      "        stop       -5.67       -5.41       -0.27\n",
      "         the       -2.38       -2.37       -0.00\n",
      "     plotted       -8.76       -9.02        0.25\n",
      "  sabotages.       -1.00       -1.00        0.00\n",
      "           <       -2.84       -2.80       -0.04\n",
      "          br       -2.84       -2.80       -0.04\n",
      "           /       -2.84       -2.80       -0.04\n",
      "           >       -2.84       -2.80       -0.04\n",
      "           <       -2.84       -2.80       -0.04\n",
      "          br       -2.84       -2.80       -0.04\n",
      "           /       -2.84       -2.80       -0.04\n",
      "           >       -2.84       -2.80       -0.04\n",
      "       There       -4.02       -3.75       -0.27\n",
      "         are       -2.86       -2.86        0.00\n",
      "         the       -2.38       -2.37       -0.00\n",
      "       usual       -5.46       -5.72        0.26\n",
      "   Hitchcock       -6.87       -7.32        0.45\n",
      "       logic       -7.43       -6.72       -0.71\n",
      "       flaws       -6.35       -6.77        0.42\n",
      "           .       -2.38       -2.37       -0.00\n",
      "         For       -4.81       -4.72       -0.09\n",
      "     example       -5.37       -5.10       -0.27\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "           a       -2.40       -2.39       -0.00\n",
      "         guy       -4.93       -4.42       -0.51\n",
      "        with       -2.67       -2.69        0.02\n",
      "   handcuffs       -9.86       -9.98        0.12\n",
      "      *frees      -10.11      -11.37        1.25\n",
      "     himself       -4.75       -4.97        0.22\n",
      "       using       -5.93       -5.64       -0.29\n",
      "           a       -2.40       -2.39       -0.00\n",
      "         car       -5.67       -5.43       -0.24\n",
      "         fan       -4.87       -4.93        0.06\n",
      "        belt       -8.42       -8.13       -0.29\n",
      "         etc       -5.65       -5.47       -0.18\n",
      "           .       -2.38       -2.37       -0.00\n",
      "           (       -2.92       -2.90       -0.02\n",
      "        *Why       -6.06       -5.03       -1.02\n",
      "        does       -3.56       -3.46       -0.10\n",
      "         n't       -2.94       -2.70       -0.23\n",
      "          he       -3.20       -3.24        0.04\n",
      "        just       -3.31       -3.03       -0.28\n",
      "       drive       -6.76       -6.60       -0.16\n",
      "        away       -4.62       -4.52       -0.10\n",
      "          in       -2.48       -2.49        0.01\n",
      "         the       -2.38       -2.37       -0.00\n",
      "         car       -5.67       -5.43       -0.24\n",
      "           ?       -3.76       -3.21       -0.55\n",
      "      Surely       -8.40       -7.70       -0.69\n",
      "   handcuffs       -9.86       -9.98        0.12\n",
      "         are       -2.86       -2.86        0.00\n",
      "         n't       -2.94       -2.70       -0.23\n",
      "        that       -2.57       -2.53       -0.04\n",
      "*restrictive       -9.58      -10.96        1.39\n",
      "           ?       -3.76       -3.21       -0.55\n",
      "          He       -4.08       -4.28        0.20\n",
      "          's       -2.65       -2.66        0.02\n",
      "        able       -5.12       -5.37        0.25\n",
      "          to       -2.43       -2.41       -0.02\n",
      "        swim       -8.25       -8.80        0.55\n",
      "          in       -2.48       -2.49        0.01\n",
      "        them       -3.76       -3.70       -0.06\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "       after       -3.90       -3.86       -0.04\n",
      "         all       -3.00       -2.97       -0.03\n",
      "           !       -3.31       -3.27       -0.04\n",
      "           )       -2.91       -2.89       -0.02\n",
      "           <       -2.84       -2.80       -0.04\n",
      "          br       -2.84       -2.80       -0.04\n",
      "           /       -2.84       -2.80       -0.04\n",
      "           >       -2.84       -2.80       -0.04\n",
      "           <       -2.84       -2.80       -0.04\n",
      "          br       -2.84       -2.80       -0.04\n",
      "           /       -2.84       -2.80       -0.04\n",
      "           >       -2.84       -2.80       -0.04\n",
      "          If       -4.04       -3.81       -0.23\n",
      "         you       -2.97       -2.89       -0.08\n",
      "        want       -4.46       -4.17       -0.29\n",
      "           a       -2.40       -2.39       -0.00\n",
      "      better       -4.11       -3.76       -0.35\n",
      "   Hitchcock       -6.87       -7.32        0.45\n",
      "    *wartime       -8.07       -9.17        1.10\n",
      "  propaganda       -7.43       -6.98       -0.45\n",
      "       flick       -5.62       -5.14       -0.48\n",
      "        from       -3.04       -3.06        0.02\n",
      "         the       -2.38       -2.37       -0.00\n",
      "          40       -6.80       -6.69       -0.11\n",
      "          's       -2.65       -2.66        0.02\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "           I       -2.63       -2.55       -0.08\n",
      "       would       -3.44       -3.20       -0.23\n",
      "      advise       -8.15       -7.37       -0.78\n",
      "         you       -2.97       -2.89       -0.08\n",
      "       watch       -3.84       -3.78       -0.06\n",
      "          ``       -3.08       -2.98       -0.10\n",
      "     Foreign       -9.02       -9.50        0.48\n",
      "Correspondant      -11.37      -11.37        0.00\n",
      "          ''       -3.08       -2.98       -0.10\n",
      "           .       -2.38       -2.37       -0.00\n",
      "        They       -4.57       -4.37       -0.19\n",
      "         are       -2.86       -2.86        0.00\n",
      "        both       -4.28       -4.75        0.47\n",
      "       silly       -6.08       -5.34       -0.75\n",
      "       chase       -6.68       -6.53       -0.16\n",
      "      movies       -3.86       -3.72       -0.14\n",
      "        with       -2.67       -2.69        0.02\n",
      "           a       -2.40       -2.39       -0.00\n",
      "      catchy       -7.69       -8.17        0.48\n",
      "      finale       -6.79       -7.03        0.24\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "         but       -2.74       -2.69       -0.05\n",
      "          ``       -3.08       -2.98       -0.10\n",
      "     Foreign       -9.02       -9.50        0.48\n",
      "Correspondant      -11.37      -11.37        0.00\n",
      "          ''       -3.08       -2.98       -0.10\n",
      "       makes       -4.08       -4.28        0.20\n",
      "       great       -3.36       -4.09        0.73\n",
      "         use       -5.01       -4.90       -0.11\n",
      "          of       -2.41       -2.41       -0.00\n",
      "   umbrellas      -11.37      -10.96       -0.41\n",
      "         and       -2.39       -2.41        0.02\n",
      "     *tulips      -10.96      -12.06        1.10\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "   something       -4.23       -3.91       -0.31\n",
      "   Spielberg       -7.67       -7.93        0.27\n",
      "        rips       -9.02       -8.05       -0.96\n",
      "         off       -4.16       -3.81       -0.35\n",
      "      nicely       -6.45       -7.30        0.85\n",
      "          in       -2.48       -2.49        0.01\n",
      "          ``       -3.08       -2.98       -0.10\n",
      "    Minority       -9.98       -9.98        0.00\n",
      "      Report       -9.50       -9.23       -0.27\n",
      "          ''       -3.08       -2.98       -0.10\n",
      "           .       -2.38       -2.37       -0.00\n",
      "           <       -2.84       -2.80       -0.04\n",
      "          br       -2.84       -2.80       -0.04\n",
      "           /       -2.84       -2.80       -0.04\n",
      "           >       -2.84       -2.80       -0.04\n",
      "           <       -2.84       -2.80       -0.04\n",
      "          br       -2.84       -2.80       -0.04\n",
      "           /       -2.84       -2.80       -0.04\n",
      "           >       -2.84       -2.80       -0.04\n",
      "     *7.5/10       -8.63      -10.96        2.34\n",
      "           -       -4.05       -4.02       -0.03\n",
      "        Some       -5.21       -5.26        0.05\n",
      "        good       -3.27       -3.23       -0.04\n",
      "         set       -4.75       -4.82        0.07\n",
      "      pieces       -6.40       -6.43        0.03\n",
      "           .       -2.38       -2.37       -0.00\n",
      "      Beyond       -7.83       -7.57       -0.25\n",
      "        that       -2.57       -2.53       -0.04\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "     however       -5.07       -5.13        0.06\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "       there       -3.45       -3.26       -0.19\n",
      "          's       -2.65       -2.66        0.02\n",
      "     nothing       -4.77       -3.98       -0.79\n",
      "        much       -3.57       -3.47       -0.11\n",
      "          to       -2.43       -2.41       -0.02\n",
      "        sink       -8.23       -7.78       -0.45\n",
      "        your       -4.14       -3.88       -0.26\n",
      "       teeth       -7.61       -6.89       -0.72\n",
      "        into       -3.61       -3.57       -0.04\n",
      "           .       -2.38       -2.37       -0.00\n"
     ]
    }
   ],
   "source": [
    "show_word_weights(FPs[0], nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word         pos         neg        diff\n",
      "================================================\n",
      "        This       -3.12       -3.11       -0.01\n",
      "       movie       -2.89       -2.72       -0.17\n",
      "          is       -2.45       -2.46        0.02\n",
      "        just       -3.31       -3.03       -0.28\n",
      "       plain       -6.64       -5.69       -0.95\n",
      "       silly       -6.08       -5.34       -0.75\n",
      "           .       -2.38       -2.37       -0.00\n",
      "      Almost       -7.45       -7.35       -0.09\n",
      "       every       -4.32       -4.32       -0.01\n",
      "       scene       -4.13       -4.03       -0.10\n",
      "         has       -3.11       -3.21        0.11\n",
      "        some       -3.32       -3.20       -0.13\n",
      "         bit       -4.39       -4.60        0.21\n",
      "          of       -2.41       -2.41       -0.00\n",
      "       humor       -5.20       -5.44        0.24\n",
      "           :       -3.74       -3.64       -0.10\n",
      "     running       -5.82       -5.39       -0.43\n",
      "        gags       -6.78       -6.82        0.04\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "   slapstick       -7.04       -7.21        0.17\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "         and       -2.39       -2.41        0.02\n",
      "       great       -3.36       -4.09        0.73\n",
      "       jokes       -5.96       -5.46       -0.49\n",
      "           .       -2.38       -2.37       -0.00\n",
      "         The       -2.69       -2.64       -0.04\n",
      "      acting       -4.05       -3.57       -0.48\n",
      "          is       -2.45       -2.46        0.02\n",
      "         n't       -2.94       -2.70       -0.23\n",
      "        that       -2.57       -2.53       -0.04\n",
      "       great       -3.36       -4.09        0.73\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "         and       -2.39       -2.41        0.02\n",
      "         the       -2.38       -2.37       -0.00\n",
      "        plot       -4.15       -3.63       -0.52\n",
      "          is       -2.45       -2.46        0.02\n",
      "      cliche       -8.35       -7.58       -0.76\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "         but       -2.74       -2.69       -0.05\n",
      "         the       -2.38       -2.37       -0.00\n",
      "       jokes       -5.96       -5.46       -0.49\n",
      "        more       -3.26       -3.31        0.05\n",
      "        than       -3.56       -3.44       -0.12\n",
      "        make       -3.81       -3.51       -0.30\n",
      "          up       -3.40       -3.31       -0.09\n",
      "         for       -2.67       -2.66       -0.01\n",
      "        that       -2.57       -2.53       -0.04\n",
      "           .       -2.38       -2.37       -0.00\n",
      "          If       -4.04       -3.81       -0.23\n",
      "         you       -2.97       -2.89       -0.08\n",
      "        have       -2.90       -2.77       -0.13\n",
      "           a       -2.40       -2.39       -0.00\n",
      "      chance       -5.31       -5.64        0.33\n",
      "          to       -2.43       -2.41       -0.02\n",
      "         see       -3.35       -3.43        0.08\n",
      "        this       -2.54       -2.47       -0.07\n",
      "       movie       -2.89       -2.72       -0.17\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "           I       -2.63       -2.55       -0.08\n",
      "   recommend       -4.71       -5.15        0.45\n",
      "        that       -2.57       -2.53       -0.04\n",
      "         you       -2.97       -2.89       -0.08\n",
      "          do       -3.31       -3.06       -0.25\n",
      "           .       -2.38       -2.37       -0.00\n"
     ]
    }
   ],
   "source": [
    "show_word_weights(FNs[0], nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use an external sample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2 = []\n",
    "with open('data/barbie_sample.tsv') as f:\n",
    "    for line in f:\n",
    "        review, label = line.strip().split('\\t')\n",
    "        doc = nltk.word_tokenize(review)\n",
    "        sample2.append((doc, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 3, FP: 0, FN: 2, TN: 5\n",
      "Precision: 100.00%, Recall: 60.00%, F1: 75.00%\n"
     ]
    }
   ],
   "source": [
    "evaluate(nb, sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPs, FNs = get_errors(nb, sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word         pos         neg        diff\n",
      "================================================\n",
      "           I       -2.63       -2.55       -0.08\n",
      "      really       -3.51       -3.40       -0.11\n",
      "     enjoyed       -4.95       -5.83        0.88\n",
      "        this       -2.54       -2.47       -0.07\n",
      "      little       -3.89       -3.95        0.06\n",
      "       thing       -4.41       -3.91       -0.50\n",
      "           !       -3.31       -3.27       -0.04\n",
      "          It       -3.04       -3.09        0.04\n",
      "          is       -2.45       -2.46        0.02\n",
      "       silly       -6.08       -5.34       -0.75\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "     kitschy       -9.86       -9.66       -0.20\n",
      "          as       -2.73       -2.82        0.09\n",
      "        hell       -6.11       -5.48       -0.63\n",
      "           -       -4.05       -4.02       -0.03\n",
      "         BUT       -7.83       -7.63       -0.20\n",
      "           !       -3.31       -3.27       -0.04\n",
      "  Definitely       -6.91       -7.41        0.50\n",
      "         FUN       -9.58      -10.11        0.54\n",
      "           !       -3.31       -3.27       -0.04\n",
      "Entertaining       -9.42      -10.27        0.85\n",
      "           !       -3.31       -3.27       -0.04\n",
      "        Dare       -9.76      -10.11        0.36\n",
      "           I       -2.63       -2.55       -0.08\n",
      "         say       -4.08       -3.86       -0.22\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "     perhaps       -5.30       -5.47        0.16\n",
      "        Even       -5.23       -4.87       -0.36\n",
      "         ...       -4.01       -3.66       -0.35\n",
      "     Thought       -9.42       -9.35       -0.07\n",
      "   provoking       -7.40       -8.23        0.83\n",
      "           ?       -3.76       -3.21       -0.55\n",
      "          It       -3.04       -3.09        0.04\n",
      "          IS       -6.62       -6.55       -0.08\n",
      "           a       -2.40       -2.39       -0.00\n",
      "          ``       -3.08       -2.98       -0.10\n",
      "        Pink       -8.13       -8.66        0.53\n",
      "  strawberry      -10.96      -10.11       -0.85\n",
      "   bubblegum      -10.27      -10.27        0.00\n",
      "          ''       -3.08       -2.98       -0.10\n",
      "     version       -5.10       -5.08       -0.02\n",
      "          of       -2.41       -2.41       -0.00\n",
      "          ``       -3.08       -2.98       -0.10\n",
      "      Social       -9.50       -9.66        0.17\n",
      "  commentary       -6.60       -6.79        0.20\n",
      "          ''       -3.08       -2.98       -0.10\n",
      "         and       -2.39       -2.41        0.02\n",
      "     perhaps       -5.30       -5.47        0.16\n",
      "        even       -3.63       -3.25       -0.38\n",
      "           a       -2.40       -2.39       -0.00\n",
      "         bit       -4.39       -4.60        0.21\n",
      "          of       -2.41       -2.41       -0.00\n",
      "      gender       -8.17       -8.56        0.40\n",
      "        role       -4.38       -4.74        0.37\n",
      "*Stereotypes      -11.37       -9.98       -1.39\n",
      "   criticism       -6.97       -7.52        0.54\n",
      "           ?       -3.76       -3.21       -0.55\n",
      "      Margot       -9.58      -10.27        0.69\n",
      "      Robbie       -9.50       -9.29       -0.21\n",
      "          is       -2.45       -2.46        0.02\n",
      "  *excellent       -4.43       -5.79        1.36\n",
      "         and       -2.39       -2.41        0.02\n",
      "        Ryan       -6.68       -7.03        0.35\n",
      "    Reynolds       -7.53       -8.02        0.49\n",
      "          is       -2.45       -2.46        0.02\n",
      "        good       -3.27       -3.23       -0.04\n",
      "          in       -2.48       -2.49        0.01\n",
      "       their       -3.49       -3.55        0.06\n",
      "  respective       -7.70       -8.53        0.83\n",
      "       roles       -5.26       -5.73        0.47\n",
      "           !       -3.31       -3.27       -0.04\n",
      "         The       -2.69       -2.64       -0.04\n",
      " scenography      -11.37      -10.96       -0.41\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "       music       -4.43       -4.79        0.36\n",
      "         and       -2.39       -2.41        0.02\n",
      "     filming       -6.50       -6.31       -0.19\n",
      "          is       -2.45       -2.46        0.02\n",
      "  *excellent       -4.43       -5.79        1.36\n",
      "           !       -3.31       -3.27       -0.04\n",
      "           I       -2.63       -2.55       -0.08\n",
      "       Would       -7.38       -7.20       -0.18\n",
      "          be       -2.92       -2.80       -0.12\n",
      "   surprised       -5.44       -5.93        0.49\n",
      "          if       -3.48       -3.25       -0.23\n",
      "        this       -2.54       -2.47       -0.07\n",
      "        film       -2.87       -2.90        0.03\n",
      "        does       -3.56       -3.46       -0.10\n",
      "         not       -2.89       -2.80       -0.09\n",
      "        reel       -8.28       -7.95       -0.33\n",
      "          in       -2.48       -2.49        0.01\n",
      "           a       -2.40       -2.39       -0.00\n",
      "      couple       -5.08       -4.92       -0.16\n",
      "          of       -2.41       -2.41       -0.00\n",
      "      awards       -7.22       -7.55        0.33\n",
      "           !       -3.31       -3.27       -0.04\n",
      "           I       -2.63       -2.55       -0.08\n",
      "       would       -3.44       -3.20       -0.23\n",
      "       truly       -4.86       -5.20        0.34\n",
      "       enjoy       -4.80       -5.20        0.41\n",
      "    watching       -4.31       -3.98       -0.34\n",
      "        THIS       -7.25       -6.61       -0.64\n",
      "    together       -4.69       -4.90        0.22\n",
      "        with       -2.67       -2.69        0.02\n",
      "        some       -3.32       -3.20       -0.13\n",
      "    children       -5.28       -5.53        0.25\n",
      "         and       -2.39       -2.41        0.02\n",
      "       young       -4.29       -4.78        0.49\n",
      "      people       -3.70       -3.61       -0.09\n",
      "         and       -2.39       -2.41        0.02\n",
      "        hear       -5.82       -5.83        0.01\n",
      "        what       -3.36       -3.29       -0.07\n",
      "        they       -3.27       -3.06       -0.21\n",
      "         say       -4.08       -3.86       -0.22\n",
      "       about       -3.18       -3.10       -0.08\n",
      "          it       -2.52       -2.50       -0.02\n",
      "           !       -3.31       -3.27       -0.04\n",
      "         AND       -6.79       -6.53       -0.26\n",
      "           I       -2.63       -2.55       -0.08\n",
      "       would       -3.44       -3.20       -0.23\n",
      "        feel       -4.46       -4.58        0.12\n",
      "     totally       -5.42       -5.09       -0.33\n",
      " comfortable       -7.27       -7.92        0.65\n",
      "        with       -2.67       -2.69        0.02\n",
      "        that       -2.57       -2.53       -0.04\n",
      "        they       -3.27       -3.06       -0.21\n",
      "       would       -3.44       -3.20       -0.23\n",
      "        have       -2.90       -2.77       -0.13\n",
      "           a       -2.40       -2.39       -0.00\n",
      "        good       -3.27       -3.23       -0.04\n",
      "        time       -3.36       -3.35       -0.01\n",
      "     without       -4.44       -4.45        0.02\n",
      "       being       -3.85       -3.82       -0.03\n",
      " overwhelmed       -8.37       -9.29        0.92\n",
      "           !       -3.31       -3.27       -0.04\n",
      "           I       -2.63       -2.55       -0.08\n",
      "      myself       -5.53       -5.19       -0.33\n",
      "    actually       -4.43       -4.06       -0.37\n",
      "       found       -4.56       -4.62        0.06\n",
      "         the       -2.38       -2.37       -0.00\n",
      "      gender       -8.17       -8.56        0.40\n",
      "*Stereotypes      -11.37       -9.98       -1.39\n",
      "      social       -6.06       -6.52        0.47\n",
      "  commentary       -6.60       -6.79        0.20\n",
      " interesting       -4.55       -4.41       -0.14\n",
      "         and       -2.39       -2.41        0.02\n",
      "     perhaps       -5.30       -5.47        0.16\n",
      "        even       -3.63       -3.25       -0.38\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "        dare       -7.43       -7.42       -0.01\n",
      "           I       -2.63       -2.55       -0.08\n",
      "         say       -4.08       -3.86       -0.22\n",
      "           a       -2.40       -2.39       -0.00\n",
      "         bit       -4.39       -4.60        0.21\n",
      "     thought       -4.46       -4.29       -0.17\n",
      "   provoking       -7.40       -8.23        0.83\n",
      "         and       -2.39       -2.41        0.02\n",
      "   inspiring       -7.26       -8.21        0.95\n",
      "           !       -3.31       -3.27       -0.04\n",
      "          IS       -6.62       -6.55       -0.08\n",
      "        this       -2.54       -2.47       -0.07\n",
      "           a       -2.40       -2.39       -0.00\n",
      "          ``       -3.08       -2.98       -0.10\n",
      "       GREAT       -7.49       -8.30        0.81\n",
      "        FILM       -8.35       -8.25       -0.09\n",
      "          ''       -3.08       -2.98       -0.10\n",
      "           ?       -3.76       -3.21       -0.55\n",
      "           -       -4.05       -4.02       -0.03\n",
      "         *NO       -7.81       -6.49       -1.33\n",
      "           .       -2.38       -2.37       -0.00\n",
      "         BUT       -7.83       -7.63       -0.20\n",
      "           !       -3.31       -3.27       -0.04\n",
      "          IT       -7.13       -6.90       -0.23\n",
      "          IS       -6.62       -6.55       -0.08\n",
      "         FUN       -9.58      -10.11        0.54\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "Entertaining       -9.42      -10.27        0.85\n",
      "         and       -2.39       -2.41        0.02\n",
      "        good       -3.27       -3.23       -0.04\n",
      "         fun       -4.50       -4.89        0.39\n",
      "         for       -2.67       -2.66       -0.01\n",
      "         the       -2.38       -2.37       -0.00\n",
      "        head       -5.31       -5.09       -0.22\n",
      "           !       -3.31       -3.27       -0.04\n",
      "           I       -2.63       -2.55       -0.08\n",
      "         CAN       -8.66       -8.23       -0.43\n",
      "      EASILY      -11.37      -10.67       -0.69\n",
      "        with       -2.67       -2.69        0.02\n",
      "        good       -3.27       -3.23       -0.04\n",
      "  conscience       -7.84       -8.40        0.56\n",
      "   recommend       -4.71       -5.15        0.45\n",
      "        this       -2.54       -2.47       -0.07\n",
      "        film       -2.87       -2.90        0.03\n",
      "          as       -2.73       -2.82        0.09\n",
      "         fun       -4.50       -4.89        0.39\n",
      "entertainment       -5.64       -5.83        0.19\n",
      "           !       -3.31       -3.27       -0.04\n",
      "           A       -3.78       -3.92        0.14\n",
      "      Kitchy       -1.00       -1.00        0.00\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "  *plasticky      -12.06      -10.96       -1.10\n",
      "    pastiche       -9.29       -9.12       -0.17\n",
      "           -       -4.05       -4.02       -0.03\n",
      "     without       -4.44       -4.45        0.02\n",
      "    becoming       -6.38       -6.71        0.33\n",
      "   *annoying       -6.42       -5.14       -1.28\n",
      "           ,       -2.40       -2.39       -0.01\n",
      "   *tiresome       -8.73       -7.39       -1.34\n",
      "          or       -3.27       -3.06       -0.21\n",
      "     *boring       -5.91       -4.56       -1.35\n",
      "           !       -3.31       -3.27       -0.04\n"
     ]
    }
   ],
   "source": [
    "show_word_weights(FNs[0], nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Milestone 2 of the Project exercise you should perform error analysis on your baseline models and discuss your findings, including implications for how your solution could be improved**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngram language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ngrams are the sequences of words in a text__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'This is the most famous movie by Aaron Sorkin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'the', 'most', 'famous', 'movie', 'by', 'Aaron', 'Sorkin']\n"
     ]
    }
   ],
   "source": [
    "unigrams = text.split()\n",
    "print(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is'], ['is', 'the'], ['the', 'most'], ['most', 'famous'], ['famous', 'movie'], ['movie', 'by'], ['by', 'Aaron'], ['Aaron', 'Sorkin']]\n"
     ]
    }
   ],
   "source": [
    "bigrams = [unigrams[i:i+2] for i in range(len(unigrams) - 1)]\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the'], ['is', 'the', 'most'], ['the', 'most', 'famous'], ['most', 'famous', 'movie'], ['famous', 'movie', 'by'], ['movie', 'by', 'Aaron'], ['by', 'Aaron', 'Sorkin']]\n"
     ]
    }
   ],
   "source": [
    "trigrams = [unigrams[i:i+3] for i in range(len(unigrams) - 2)]\n",
    "print(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-ngrams are a natural extension of bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of ngrams is also characteristic of a language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngram frequencies in a language can be used to approximate the __probability of any text__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is known as __ngram language modeling__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why model the probability of strings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- autocomplete, autocorrect\n",
    "- speech recognition\n",
    "- machine translation\n",
    "- augmentative and alternative communication (AAC)\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the probability of an upcoming word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(w_i | w_1, w_2, \\ldots, w_{i-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ngram language model __approximates__ this with the last $n-1$ words, i.e. by looking at a window of $n$ words only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigram language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(w_i | w_{i-2}, w_{i-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be approximated based on counts of trigrams and bigrams in a corpus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(w_i | w_{i-2}, w_{i-1}) \\sim \\frac{c(w_{i-2}, w_{i-1}, w_{i})}{c(w_{i-2}, w_{i-1})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The larger the value of $n$, the higher the number of parameters, and even very large corpora become small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "124225it [00:19, 6410.66it/s]\n"
     ]
    }
   ],
   "source": [
    "trigrams = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "with open('data/shakespeare.txt') as f:\n",
    "    for line in tqdm(f):\n",
    "        words = tuple(['<s>'] + [w.lower() for w in nltk.word_tokenize(line)] + ['</s>'])\n",
    "        for i in range(len(words) - 2):\n",
    "            w1, w2, w3 = words[i:i+3]\n",
    "            trigrams[w1][w2][w3] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_word(words):\n",
    "    assert len(words) >= 2\n",
    "    w1, w2 = words[-2].lower(), words[-1].lower()\n",
    "    words, freqs = zip(*trigrams[w1][w2].items())\n",
    "    p = np.array(freqs) / sum(freqs)\n",
    "    return np.random.choice(words, p=p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'now'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_word(('<s>', 'I'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(first_word):\n",
    "    text = ['<s>', first_word.lower()]\n",
    "    while text[-1] != '</s>':\n",
    "        text.append(get_next_word(text))\n",
    "    return ' '.join(text[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'she hath but wrong to wake northumberland and the will .'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence('She')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_\"If oneâs view of language is that it is a probability\n",
    "distribution over strings of letter or sounds, one turns oneâs back on the scientific\n",
    "achievements of the ages and foreswears the opportunity that computers offer to carry\n",
    "that enterprise forward.\"_ [(Kay 2006, p.430)](https://aclanthology.org/J05-4001.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
